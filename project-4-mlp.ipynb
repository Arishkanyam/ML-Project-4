{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nThe evaluation API requires that you set up a server which will respond to inference requests.\nWe have already defined the server; you just need write the predict function.\nWhen we evaluate your submission on the hidden test set the client defined in `default_gateway` will run in a different container\nwith direct access to the hidden test set and hand off the data timestep by timestep.\n\nYour code will always have access to the published copies of the copmetition files.\n\"\"\"\n\nimport os\n\nimport pandas as pd\nimport polars as pl\nimport numpy as np\n\nimport kaggle_evaluation.default_inference_server\n\ntrain_path = \"/kaggle/input/hull-tactical-market-prediction/train.csv\"\ntest_path = \"/kaggle/input/hull-tactical-market-prediction/test.csv\"\n\ntrain = pd.read_csv(train_path)\ntest_test = pd.read_csv(test_path)\n\nprint(train.columns)\nprint(train.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:19:17.694123Z","iopub.execute_input":"2025-12-04T16:19:17.694990Z","iopub.status.idle":"2025-12-04T16:19:17.941170Z","shell.execute_reply.started":"2025-12-04T16:19:17.694954Z","shell.execute_reply":"2025-12-04T16:19:17.939999Z"}},"outputs":[{"name":"stdout","text":"Index(['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1',\n       'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19',\n       'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3',\n       'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13',\n       'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7',\n       'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5',\n       'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4',\n       'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2',\n       'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'forward_returns',\n       'risk_free_rate', 'market_forward_excess_returns'],\n      dtype='object')\n(9021, 98)\n","output_type":"stream"}],"execution_count":167},{"cell_type":"markdown","source":"Step1 - Baseline model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nbaseline_model = LinearRegression()\nfeature_cols = [c for c in train.columns if c.startswith(('M','E','I','P','V','S','MOM'))]\nX_train = train[feature_cols].fillna(0)\ny_train = train['market_forward_excess_returns']\n\ndef baseline_train():\n    baseline_model.fit(X_train, y_train)\n\n#baseline_train()\n\ndef baseline_predict(test: pl.DataFrame) -> pl.DataFrame:\n    test_df = test.select(feature_cols).to_pandas().fillna(0)\n    preds = baseline_model.predict(test_df)\n    preds = np.clip(preds, 0, 2)\n    preds = np.nan_to_num(preds, 0.0)\n\n    test = test.with_columns(\n        prediction = pl.Series(preds)\n    )\n    return test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:19:17.942900Z","iopub.execute_input":"2025-12-04T16:19:17.943322Z","iopub.status.idle":"2025-12-04T16:19:17.970436Z","shell.execute_reply.started":"2025-12-04T16:19:17.943269Z","shell.execute_reply":"2025-12-04T16:19:17.969240Z"}},"outputs":[],"execution_count":168},{"cell_type":"markdown","source":"Step2 - Model Development\n\nHere i use LightGBM, who seems to be more useful and fast for this task and it give a better score than just the baseline","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n\nlgb_train = lgb.Dataset(X_train, y_train)\nparams = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'verbose': -1\n}\nlgb_model = lgb.train(params, lgb_train, num_boost_round=500)\n\ndef lgb_predict(test: pl.DataFrame):\n    X_test = test.select(feature_cols).to_pandas().fillna(0)\n    preds = lgb_model.predict(X_test)\n    preds = np.clip(preds, 0, 2)\n    test = test.with_columns(prediction=pl.Series(preds))\n    return test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:19:17.971516Z","iopub.execute_input":"2025-12-04T16:19:17.971926Z","iopub.status.idle":"2025-12-04T16:19:22.941744Z","shell.execute_reply.started":"2025-12-04T16:19:17.971889Z","shell.execute_reply":"2025-12-04T16:19:22.940735Z"}},"outputs":[],"execution_count":169},{"cell_type":"markdown","source":"Time series models \n\nscaling centered around 1 (z-score) → exploits relative variations in predictions, controls volatility.\n\nalso cap volatility to 120%","metadata":{}},{"cell_type":"code","source":"models = []\n\n\ndef lgb_train_time_series(train, feature_cols):\n    min_id = int(train[\"date_id\"].min())\n    max_id = int(train[\"date_id\"].max())\n\n    window = 3000\n    step = 500\n\n    start = min_id\n    splits = []\n\n    while start + window <= max_id:\n        train_start = start\n        train_end = start + window - 1\n        val_start = train_end + 1\n        val_end = min(train_end + step, max_id)\n        splits.append((train_start, train_end, val_start, val_end))\n        start += step\n\n    # Train a model for each split\n    for train_start, train_end, val_start, val_end in splits:\n        train_df = train[(train[\"date_id\"] >= train_start) & (train[\"date_id\"] <= train_end)]\n        val_df   = train[(train[\"date_id\"] >= val_start) & (train[\"date_id\"] <= val_end)]\n\n        X_train = train_df[feature_cols].fillna(0)\n        y_train = train_df[\"market_forward_excess_returns\"].fillna(0)\n\n        X_val = val_df[feature_cols].fillna(0)\n        y_val = val_df[\"market_forward_excess_returns\"].fillna(0)\n\n        model = lgb.LGBMRegressor(\n            n_estimators=300,\n            learning_rate=0.05,\n            num_leaves=31\n        )\n\n        model.fit(X_train, y_train)\n        models.append(model)\n\n# cap to 120%\ndef cap_volatility(test: pl.DataFrame, preds):\n    market_returns = test[\"lagged_forward_returns\"].to_numpy()\n    risk_free_rate = test[\"lagged_risk_free_rate\"].to_numpy()\n    market_vol = np.std(market_returns)\n    strategy_returns = risk_free_rate * (1 - preds) + preds * market_returns\n    strategy_vol = np.std(strategy_returns)\n    max_vol = 1.2 * market_vol\n    if strategy_vol > max_vol:\n        scaling_factor = max_vol / strategy_vol\n        preds = preds * scaling_factor\n    return preds\n\ndef lgb_predict_time_series(test: pl.DataFrame, feature_cols):\n    X_test = test.select(feature_cols).to_pandas().fillna(0)\n\n    preds_all = [m.predict(X_test) for m in models]\n    preds = np.mean(preds_all, axis=0)\n\n    #scaling centered around 1 (predict allocations weight)\n    mean_pred = np.mean(preds)\n    std_pred = np.std(preds)\n    if std_pred > 0:\n        preds_scaled = 1 + (preds - mean_pred) / std_pred\n    else:\n        preds_scaled = np.ones_like(preds)\n        \n    preds_capped = cap_volatility(test, preds_scaled)\n    preds_clipped = np.clip(preds_capped, 0, 2)\n    \n    test = test.with_columns(\n        pl.Series(\"prediction\", preds_clipped.astype(np.float32))\n    )\n    \n    return test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"step4: feature engineering","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef add_other_features(data, isTest: bool):\n    forward_returns_label = 'lagged_forward_returns' if isTest  else 'forward_returns'\n    data['momentum_5'] = data[forward_returns_label].rolling(5, min_periods=1).mean()\n    data['volatility_10'] = data[forward_returns_label].rolling(10, min_periods=1).std()\n    rolling_max = data[forward_returns_label].rolling(20, min_periods=1).max()\n    data['drawdown_20'] = (rolling_max - data[forward_returns_label]) / rolling_max.replace(0, np.nan)\n    data['drawdown_20'] = data[forward_returns_label].fillna(0)\n    return data\n\ntrain = add_other_features(train, False)\nall_features = feature_cols + ['momentum_5', 'volatility_10', 'drawdown_20']\ncorr_target = train[all_features + [\"market_forward_excess_returns\"]].corr()[\"market_forward_excess_returns\"].sort_values(ascending=False)\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x=corr_target.values, y=corr_target.index)\nplt.title(\"Corrélation de chaque feature (anciennes + nouvelles) avec la target\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncorr_target = train[all_features + [\"market_forward_excess_returns\"]].corr()[\"market_forward_excess_returns\"]\n\nseuil = 0.03\n\nselected_features = corr_target[abs(corr_target) >= seuil].index.tolist()\n\nselected_features = [f for f in selected_features if f != \"market_forward_excess_returns\"]\n\nprint(\"features selected :\", selected_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_train_time_series(train, selected_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pandas -> Polars\ndef pd_to_pl(df: pd.DataFrame) -> pl.DataFrame:\n    return pl.from_pandas(df)\n\n# Polars -> Pandas\ndef pl_to_pd(df: pl.DataFrame) -> pd.DataFrame:\n    return df.to_pandas()\n    \ndef predict(test: pl.DataFrame) -> pl.DataFrame:\n    test = add_other_features(pl_to_pd(test), True)\n    return lgb_predict_time_series(pd_to_pl(test), selected_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:19:32.433472Z","iopub.execute_input":"2025-12-04T16:19:32.433882Z","iopub.status.idle":"2025-12-04T16:19:32.440193Z","shell.execute_reply.started":"2025-12-04T16:19:32.433854Z","shell.execute_reply":"2025-12-04T16:19:32.439057Z"}},"outputs":[],"execution_count":170},{"cell_type":"code","source":"# When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting\n# or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very\n# first `predict` call, which does not have the usual 1 minute response deadline.\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}